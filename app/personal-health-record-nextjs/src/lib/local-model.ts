// This is a mock implementation - in the actual app, you would use @mlc-ai/web-llm
// We're providing this placeholder to maintain compatibility with components that might use it

class LocalModel {
  private initialized: boolean = false;

  async initialize() {
    if (!this.initialized) {
      console.log('Mock local model initialization...');
      // In a real implementation, this would load the actual model
      this.initialized = true;
      console.log('Mock model initialization completed');
    }
  }

  async chat(prompt: string, systemPrompt: string = '') {
    await this.initialize();
    
    console.log('Using mock local model instead of actual local inference');
    
    // This is where the actual model would process the prompt
    // For now, we return a placeholder response
    return `[This is a local model placeholder response. In the actual app, this would be generated by a local LLM model.]
    
    You asked: ${prompt}
    
    When the real model is integrated, it will process this prompt with the system context: "${systemPrompt || 'default medical context'}"`;
  }

  async streamChat(
    prompt: string, 
    callback: (chunk: string, fullResponse: string) => void,
    systemPrompt: string = ''
  ) {
    await this.initialize();
    
    console.log('Using mock local model streaming instead of actual local inference');
    
    // Break the response into chunks to simulate streaming
    const fullResponse = `[This is a local model placeholder streaming response. In the actual app, this would be generated by a local LLM model.]
    
    You asked: ${prompt}
    
    When the real model is integrated, it will process this prompt with the system context: "${systemPrompt || 'default medical context'}"`;
    
    // Simulate streaming by breaking the response into words
    const words = fullResponse.split(' ');
    let currentResponse = '';
    
    // Simulate streaming with delays
    for (const word of words) {
      await new Promise(resolve => setTimeout(resolve, 50)); // Simulate thinking time
      currentResponse += word + ' ';
      callback(word + ' ', currentResponse);
    }
    
    return fullResponse;
  }

  async cleanup() {
    if (this.initialized) {
      console.log('Mock model cleanup');
      this.initialized = false;
    }
  }
}

// Create a singleton instance
const localModel = new LocalModel();
export default localModel;